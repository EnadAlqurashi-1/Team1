# -*- coding: utf-8 -*-
"""Evaluation_Metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sf3E344HY98Hm38cGtDNj1E0G3iS1C00
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install git+https://github.com/openai/CLIP.git
!pip install torch torchvision
!pip install numpy pillow

import os
import clip
import torch
from PIL import Image

# Check for GPU availability
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Define the dataset path
base_path = "/content/drive/MyDrive/CAP_6411_COMPUTER_VISION_SYSTEMS/final_project/data"

# Define your prompts
prompts = [
    "a crab, low poly",
    "a bald eagle carved out of wood",
    "delicious hamburger"
]

# Iterate through each prompt to compute similarity with the corresponding images
for prompt in prompts:
    # Tokenize the text prompt
    text_tokens = clip.tokenize([prompt]).to(device)

    # Define paths to images
    dreamfusion_front_path = os.path.join(base_path, "dreamfusion", prompt, "front.png")
    dreamfusion_back_path = os.path.join(base_path, "dreamfusion", prompt, "back.png")
    ourpipeline_front_path = os.path.join(base_path, "ourpipeline", prompt, "front.jpg")
    ourpipeline_back_path = os.path.join(base_path, "ourpipeline", prompt, "back.jpg")

    # Function to load and preprocess images
    def load_and_preprocess_image(image_path):
        image = Image.open(image_path).convert("RGB")
        return preprocess(image).unsqueeze(0).to(device)

    # Load and preprocess front and back images for both ourpipeline and dreamfusion
    dreamfusion_front_image_input = load_and_preprocess_image(dreamfusion_front_path)
    dreamfusion_back_image_input = load_and_preprocess_image(dreamfusion_back_path)
    ourpipeline_front_image_input = load_and_preprocess_image(ourpipeline_front_path)
    ourpipeline_back_image_input = load_and_preprocess_image(ourpipeline_back_path)

    with torch.no_grad():
        # Encode text
        text_features = model.encode_text(text_tokens)
        text_features /= text_features.norm(dim=-1, keepdim=True)

        # Encode dreamfusion front and back images
        dreamfusion_front_features = model.encode_image(dreamfusion_front_image_input)
        dreamfusion_back_features = model.encode_image(dreamfusion_back_image_input)
        dreamfusion_front_features /= dreamfusion_front_features.norm(dim=-1, keepdim=True)
        dreamfusion_back_features /= dreamfusion_back_features.norm(dim=-1, keepdim=True)

        # Encode ourpipeline front and back images
        ourpipeline_front_features = model.encode_image(ourpipeline_front_image_input)
        ourpipeline_back_features = model.encode_image(ourpipeline_back_image_input)
        ourpipeline_front_features /= ourpipeline_front_features.norm(dim=-1, keepdim=True)
        ourpipeline_back_features /= ourpipeline_back_features.norm(dim=-1, keepdim=True)

    # Compute cosine similarity between the text and each image
    dreamfusion_front_similarity = (dreamfusion_front_features @ text_features.T).item()
    dreamfusion_back_similarity = (dreamfusion_back_features @ text_features.T).item()
    ourpipeline_front_similarity = (ourpipeline_front_features @ text_features.T).item()
    ourpipeline_back_similarity = (ourpipeline_back_features @ text_features.T).item()

    # Average similarity for both methods
    dreamfusion_average_similarity = (dreamfusion_front_similarity + dreamfusion_back_similarity) / 2
    ourpipeline_average_similarity = (ourpipeline_front_similarity + ourpipeline_back_similarity) / 2

    # Print only average results for each prompt with metric name
    print(f"--- Prompt: '{prompt}' ---")
    print(f"DreamFusion Average CLIP Cosine Similarity: {dreamfusion_average_similarity:.4f}")
    print(f"OurPipeline Average CLIP Cosine Similarity: {ourpipeline_average_similarity:.4f}")
    print("\n")

import torch
import clip
from PIL import Image

# Load the CLIP model and set up the device (this part has been done previously)
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Function to load and preprocess images
def load_and_preprocess_image(image_path):
    if os.path.exists(image_path):
        image = Image.open(image_path).convert("RGB")
        return preprocess(image).unsqueeze(0).to(device)
    return None

# Iterate through each prompt to compute pairwise similarity between front and back
for prompt in prompts:
    # Define paths to front and back images for both methods
    dreamfusion_front_path = os.path.join(base_path, "dreamfusion", prompt, "front.png")
    dreamfusion_back_path = os.path.join(base_path, "dreamfusion", prompt, "back.png")
    ourpipeline_front_path = os.path.join(base_path, "ourpipeline", prompt, "front.jpg")
    ourpipeline_back_path = os.path.join(base_path, "ourpipeline", prompt, "back.jpg")

    # Load images
    dreamfusion_front = load_and_preprocess_image(dreamfusion_front_path)
    dreamfusion_back = load_and_preprocess_image(dreamfusion_back_path)
    ourpipeline_front = load_and_preprocess_image(ourpipeline_front_path)
    ourpipeline_back = load_and_preprocess_image(ourpipeline_back_path)

    with torch.no_grad():
        # Calculate Pairwise Similarity for DreamFusion
        if dreamfusion_front is not None and dreamfusion_back is not None:
            dreamfusion_front_features = model.encode_image(dreamfusion_front)
            dreamfusion_back_features = model.encode_image(dreamfusion_back)
            # Normalize the feature vectors before calculating cosine similarity
            dreamfusion_front_features /= dreamfusion_front_features.norm(dim=-1, keepdim=True)
            dreamfusion_back_features /= dreamfusion_back_features.norm(dim=-1, keepdim=True)
            dreamfusion_pairwise_similarity = (dreamfusion_front_features @ dreamfusion_back_features.T).item()
            print(f"--- Pairwise Similarity for DreamFusion: '{prompt}' ---")
            print(f"Front vs Back Cosine Similarity: {dreamfusion_pairwise_similarity:.4f}\n")

        # Calculate Pairwise Similarity for OurPipeline
        if ourpipeline_front is not None and ourpipeline_back is not None:
            ourpipeline_front_features = model.encode_image(ourpipeline_front)
            ourpipeline_back_features = model.encode_image(ourpipeline_back)
            # Normalize the feature vectors before calculating cosine similarity
            ourpipeline_front_features /= ourpipeline_front_features.norm(dim=-1, keepdim=True)
            ourpipeline_back_features /= ourpipeline_back_features.norm(dim=-1, keepdim=True)
            ourpipeline_pairwise_similarity = (ourpipeline_front_features @ ourpipeline_back_features.T).item()
            print(f"--- Pairwise Similarity for OurPipeline: '{prompt}' ---")
            print(f"Front vs Back Cosine Similarity: {ourpipeline_pairwise_similarity:.4f}\n")

import lpips
import torch
from PIL import Image
import torchvision.transforms as transforms

# Load LPIPS model
lpips_model = lpips.LPIPS(net='alex').to(device)

# Define image preprocessing with resizing for LPIPS evaluation
resize_transform = transforms.Compose([
    transforms.Resize((256, 256)),  # Resize both images to 256x256 pixels
    transforms.ToTensor()
])

# Function to load, resize, and preprocess images for LPIPS evaluation
def load_image_lpips(image_path):
    if os.path.exists(image_path):
        img = Image.open(image_path).convert('RGB')
        img = resize_transform(img).unsqueeze(0).to(device)
        return img
    return None

# Iterate through each prompt to compute LPIPS for perceptual similarity between DreamFusion and OurPipeline
for prompt in prompts:
    # Define paths to the front and back images of both methods
    dreamfusion_front_path = os.path.join(base_path, "dreamfusion", prompt, "front.png")
    ourpipeline_front_path = os.path.join(base_path, "ourpipeline", prompt, "front.jpg")
    dreamfusion_back_path = os.path.join(base_path, "dreamfusion", prompt, "back.png")
    ourpipeline_back_path = os.path.join(base_path, "ourpipeline", prompt, "back.jpg")

    # Load and preprocess images
    dreamfusion_front = load_image_lpips(dreamfusion_front_path)
    ourpipeline_front = load_image_lpips(ourpipeline_front_path)
    dreamfusion_back = load_image_lpips(dreamfusion_back_path)
    ourpipeline_back = load_image_lpips(ourpipeline_back_path)

    with torch.no_grad():
        # Calculate LPIPS for the front view if images exist
        if dreamfusion_front is not None and ourpipeline_front is not None:
            lpips_score_front = lpips_model(dreamfusion_front, ourpipeline_front)
            print(f"--- LPIPS for Front View: '{prompt}' ---")
            print(f"Perceptual Similarity Score (lower is better): {lpips_score_front.item():.4f}\n")

        # Calculate LPIPS for the back view if images exist
        if dreamfusion_back is not None and ourpipeline_back is not None:
            lpips_score_back = lpips_model(dreamfusion_back, ourpipeline_back)
            print(f"--- LPIPS for Back View: '{prompt}' ---")
            print(f"Perceptual Similarity Score (lower is better): {lpips_score_back.item():.4f}\n")



